{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded102b",
   "metadata": {},
   "source": [
    "# Task 2b: Abstractive Text Summarization\n",
    "\n",
    "In this notebook, you manage challenges arising in large document summarization - input text can exceed model context lengths, generate hallucinated outputs, or trigger out-of-memory errors.\n",
    "\n",
    "To mitigate these issues, this notebook demonstrates an architecture using prompt chunking and chaining with the [LangChain](https://python.langchain.com/docs/get_started/introduction.html) framework, a toolkit enabling applications leveraging language models.\n",
    "\n",
    "You explore an approach addressing scenarios when user documents surpass token limits. Chunking splits documents into segments under context length thresholds before sequentially feeding them to models. This chains prompts across chunks, retaining prior context. You apply this approach to summarize call transcripts, meetings transcripts, books, articles, blog posts, and other relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1eaf9",
   "metadata": {},
   "source": [
    "## Task 2b.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment and create a Bedrock client that automatically detects your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f0f9067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:06.039894Z",
     "iopub.status.busy": "2025-09-28T02:52:06.038825Z",
     "iopub.status.idle": "2025-09-28T02:52:06.060209Z",
     "shell.execute_reply": "2025-09-28T02:52:06.059608Z",
     "shell.execute_reply.started": "2025-09-28T02:52:06.039858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "# AWS and Bedrock imports\n",
    "import boto3\n",
    "\n",
    "# Get the region programmatically\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name or \"us-east-1\"  # Default to us-east-1 if region not set\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae9a41",
   "metadata": {},
   "source": [
    "## Task 2b.2: Summarize long text \n",
    "\n",
    "### Configuring LangChain with Boto3\n",
    "\n",
    "In this task, you specify the LLM for the LangChain Bedrock class, and pass arguments for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93df2442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:09.786865Z",
     "iopub.status.busy": "2025-09-28T02:52:09.786567Z",
     "iopub.status.idle": "2025-09-28T02:52:09.798693Z",
     "shell.execute_reply": "2025-09-28T02:52:09.797770Z",
     "shell.execute_reply.started": "2025-09-28T02:52:09.786844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "# Base LLM configuration\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "class NovaLiteWrapper(LLM):\n",
    "    \"\"\"Wrapper for Nova Lite model that formats inputs correctly.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nova-lite-wrapper\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Format prompt for Nova Lite and process.\"\"\"\n",
    "        # Format the prompt for Nova Lite's expected message structure\n",
    "        formatted_input = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": prompt}]  # Content must be an array with text objects\n",
    "                }\n",
    "            ],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 2048,\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call Bedrock directly with the properly formatted input\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=modelId,\n",
    "            body=json.dumps(formatted_input)\n",
    "        )\n",
    "        \n",
    "        # Parse the response - updated to handle Nova Lite's response format\n",
    "        response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "        \n",
    "        # Extract the text from the response\n",
    "        if 'output' in response_body and 'message' in response_body['output']:\n",
    "            message = response_body['output']['message']\n",
    "            if 'content' in message and isinstance(message['content'], list):\n",
    "                # Extract text from each content item\n",
    "                texts = []\n",
    "                for content_item in message['content']:\n",
    "                    if isinstance(content_item, dict) and 'text' in content_item:\n",
    "                        texts.append(content_item['text'])\n",
    "                return ' '.join(texts)\n",
    "        \n",
    "        # Fallback if the response format is different\n",
    "        return str(response_body)\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_id\": modelId}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count - Nova Lite uses roughly 1 token per 4 characters.\"\"\"\n",
    "        return len(text) // 4  # Rough approximation\n",
    "\n",
    "# Create the Nova Lite wrapper\n",
    "llm = NovaLiteWrapper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d423aa-cb67-4170-afdb-b037f2531921",
   "metadata": {},
   "source": [
    "## Creating a Resource-Optimized LLM Wrapper\n",
    "\n",
    "To handle Bedrock service quotas effectively, we'll create a wrapper class that optimizes resource usage and implements exponential backoff with jitter for API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a87119b-bf9d-4bec-be54-1efbcc9e8edf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:10.540144Z",
     "iopub.status.busy": "2025-09-28T02:52:10.539882Z",
     "iopub.status.idle": "2025-09-28T02:52:10.550117Z",
     "shell.execute_reply": "2025-09-28T02:52:10.549256Z",
     "shell.execute_reply.started": "2025-09-28T02:52:10.540124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced resource-optimized LLM wrapper with exponential backoff\n",
    "class ResourceOptimizedLLM(LLM):\n",
    "    \"\"\"Wrapper that optimizes resource usage for LLM processing.\"\"\"\n",
    "    \n",
    "    llm: Any  # The base LLM to wrap\n",
    "    min_pause: float = 30.0  # Minimum pause between requests\n",
    "    max_pause: float = 60.0  # Maximum pause after throttling\n",
    "    initial_pause: float = 10.0  # Initial pause between requests\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"optimized-{self.llm._llm_type}\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Process with resource optimization and exponential backoff.\"\"\"\n",
    "        # Always pause between requests to optimize resource usage\n",
    "        time.sleep(self.initial_pause)\n",
    "        \n",
    "        # Implement retry with exponential backoff\n",
    "        max_retries = 10  # More retries for important operations\n",
    "        base_delay = self.min_pause\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Making API call (attempt {attempt+1}/{max_retries})...\")\n",
    "                return self.llm._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                \n",
    "                # Handle different types of service exceptions\n",
    "                if any(err in error_str for err in [\"ThrottlingException\", \"TooManyRequests\", \"Rate exceeded\"]):\n",
    "                    if attempt < max_retries - 1:\n",
    "                        # Calculate backoff with jitter to prevent request clustering\n",
    "                        jitter = random.random() * 0.5\n",
    "                        wait_time = min(base_delay * (2 ** attempt) + jitter, self.max_pause)\n",
    "                        \n",
    "                        print(f\"Service capacity reached. Backing off for {wait_time:.2f} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(\"Maximum retries reached. Consider reducing batch size or increasing delays.\")\n",
    "                        raise\n",
    "                else:\n",
    "                    # For non-capacity errors, don't retry\n",
    "                    print(f\"Non-capacity error: {error_str}\")\n",
    "                    raise\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {**self.llm._identifying_params, \"initial_pause\": self.initial_pause}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Pass through token counting to the base model.\"\"\"\n",
    "        return self.llm.get_num_tokens(text)\n",
    "\n",
    "# Create the resource-optimized LLM\n",
    "resource_optimized_llm = ResourceOptimizedLLM(llm=llm, initial_pause=10.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b8886-b2e9-4239-8556-1f1483e9bfef",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** This wrapper adds important features for production use:\n",
    "\n",
    "- Automatic pausing between requests to respect service quotas\n",
    "- Exponential backoff with jitter for handling throttling exceptions\n",
    "- Comprehensive error handling and reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31223056",
   "metadata": {},
   "source": [
    "## Task 2b.3: Loading a text file with many tokens\n",
    "\n",
    "In this task, you use a copy of [Amazon's CEO letter to shareholders in 2022](https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2022-letter-to-shareholders) in the letters directory. You create a function to load the text file and handle potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c70352ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:11.794006Z",
     "iopub.status.busy": "2025-09-28T02:52:11.793743Z",
     "iopub.status.idle": "2025-09-28T02:52:11.799804Z",
     "shell.execute_reply": "2025-09-28T02:52:11.799182Z",
     "shell.execute_reply.started": "2025-09-28T02:52:11.793986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded successfully with 8109 tokens\n"
     ]
    }
   ],
   "source": [
    "# Document loading function\n",
    "def load_document(file_path):\n",
    "    \"\"\"Load document from file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "shareholder_letter = \"../letters/2022-letter.txt\"\n",
    "letter = load_document(shareholder_letter)\n",
    "\n",
    "if letter:\n",
    "    num_tokens = resource_optimized_llm.get_num_tokens(letter)\n",
    "    print(f\"Document loaded successfully with {num_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0e622",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** You can safely ignore the warnings and proceed to next cell. We'll address this by chunking the document in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec39d",
   "metadata": {},
   "source": [
    "## Task 2b.4: Splitting the long text into chunks\n",
    "\n",
    "In this task, you split the text into smaller chunks because it is too long to fit in the prompt. `RecursiveCharacterTextSplitter` in LangChain supports splitting long text into chunks recursively until the size of each chunk becomes smaller than chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e7c372b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:12.621313Z",
     "iopub.status.busy": "2025-09-28T02:52:12.621030Z",
     "iopub.status.idle": "2025-09-28T02:52:12.626941Z",
     "shell.execute_reply": "2025-09-28T02:52:12.626094Z",
     "shell.execute_reply.started": "2025-09-28T02:52:12.621290Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 10 chunks\n",
      "Now we have 10 documents and the first one has 543 tokens\n"
     ]
    }
   ],
   "source": [
    "# Document chunking with conservative settings\n",
    "def chunk_document(text, chunk_size=4000, chunk_overlap=200):\n",
    "    \"\"\"Split document into manageable chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    print(f\"Document split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Split the document into chunks\n",
    "if letter:\n",
    "    docs = chunk_document(letter, chunk_size=4000, chunk_overlap=200)\n",
    "    \n",
    "    if docs:\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = resource_optimized_llm.get_num_tokens(docs[0].page_content)\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acedb37-52f7-40ac-ae70-cede47b270a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The `chunk_size` parameter controls how large each chunk will be. Larger chunks provide more context but require more processing resources. The `chunk_overlap` parameter ensures some continuity between chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8ae45",
   "metadata": {},
   "source": [
    "## Task 2b.5: Summarizing chunks and combining them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d49f5",
   "metadata": {},
   "source": [
    "In this task, you implement two approaches for summarizing chunked documents: using LangChain's built-in summarize chain and a custom manual implementation that provides better control over resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc48123-d9f4-4275-a509-4ed0a687d547",
   "metadata": {},
   "source": [
    "## Understanding Implementation Approaches\n",
    "\n",
    "This notebook demonstrates two different approaches for summarizing large documents with AWS Bedrock:\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** We've included both a standard LangChain implementation and a custom implementation to show the tradeoffs between convenience and control when building production applications.\n",
    "\n",
    "### Two Paths to the Same Goal\n",
    "\n",
    "1. **Standard LangChain Implementation** (`process_documents_with_pacing`):\n",
    "   - Uses LangChain's built-in summarization chains\n",
    "   - Requires less code and is easier to implement\n",
    "   - Abstracts away the underlying complexity\n",
    "   - Great for rapid prototyping and simple use cases\n",
    "\n",
    "2. **Custom Refine Implementation** (`manual_refine_with_optimization`):\n",
    "   - Builds the refinement process step-by-step\n",
    "   - Provides complete visibility into prompts and processing\n",
    "   - Offers granular error handling for each document chunk\n",
    "   - Allows precise control over API call timing and retry logic\n",
    "\n",
    "While both achieve the same end result, the custom implementation gives you more control over the entire process, which is crucial when working with service quotas and building production-ready applications.\n",
    "\n",
    "In real-world scenarios, you might start with the standard implementation during development and then move to a custom implementation when you need more control over resource usage, error handling, or prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a2fa5-a989-4330-ad46-72ddc631b7b8",
   "metadata": {},
   "source": [
    "### Standard LangChain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e56f7535-31cf-4809-ba1b-a7ef5e3196a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:14.574665Z",
     "iopub.status.busy": "2025-09-28T02:52:14.574390Z",
     "iopub.status.idle": "2025-09-28T02:52:14.580410Z",
     "shell.execute_reply": "2025-09-28T02:52:14.579385Z",
     "shell.execute_reply.started": "2025-09-28T02:52:14.574643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom document processing with controlled pacing\n",
    "def process_documents_with_pacing(docs, chain_type=\"refine\", verbose=True):\n",
    "    \"\"\"Process documents with pacing to optimize resource usage.\"\"\"\n",
    "    \n",
    "    # Configure the chain\n",
    "    summary_chain = load_summarize_chain(\n",
    "        llm=resource_optimized_llm,\n",
    "        chain_type=chain_type,  # \"refine\" processes sequentially, good for resource optimization\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Process with additional error handling\n",
    "    try:\n",
    "        result = summary_chain.invoke(docs)\n",
    "        return result\n",
    "    except ValueError as error:\n",
    "        if \"AccessDeniedException\" in str(error):\n",
    "            print(f\"\\n\\033[91mAccess Denied: {error}\\033[0m\")\n",
    "            print(\"\\nTo troubleshoot this issue, please check:\")\n",
    "            print(\"1. Your IAM permissions for Bedrock\")\n",
    "            print(\"2. Model access permissions\")\n",
    "            print(\"3. AWS credentials configuration\")\n",
    "            return {\"output_text\": \"Error: Access denied. Check permissions.\"}\n",
    "        else:\n",
    "            print(f\"\\n\\033[91mError during processing: {error}\\033[0m\")\n",
    "            return {\"output_text\": f\"Error during processing: {str(error)}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988cc73-0982-4b69-88cb-7066176d29b5",
   "metadata": {},
   "source": [
    "### Custom Refine Implementation with Enhanced Resource Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9216651f-0fc4-42e6-93c8-8adf65b729a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:15.288307Z",
     "iopub.status.busy": "2025-09-28T02:52:15.288040Z",
     "iopub.status.idle": "2025-09-28T02:52:15.294622Z",
     "shell.execute_reply": "2025-09-28T02:52:15.294119Z",
     "shell.execute_reply.started": "2025-09-28T02:52:15.288287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manual implementation of resource-optimized processing for refine chain\n",
    "def manual_refine_with_optimization(docs, llm, verbose=True):\n",
    "    \"\"\"Manually implement refine chain with resource optimization.\"\"\"\n",
    "    if not docs:\n",
    "        return {\"output_text\": \"No documents to process.\"}\n",
    "    \n",
    "    # Process first document to get initial summary\n",
    "    print(f\"Processing initial document (1/{len(docs)})...\")\n",
    "    \n",
    "    # Simple prompt for initial document\n",
    "    initial_prompt = \"\"\"Write a concise summary of the following:\n",
    "    \"{text}\"\n",
    "    CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "    # Process first document\n",
    "    try:\n",
    "        current_summary = llm(initial_prompt.format(text=docs[0].page_content))\n",
    "        print(\"Initial summary created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating initial summary: {e}\")\n",
    "        return {\"output_text\": \"Failed to create initial summary.\"}\n",
    "    \n",
    "    # Process remaining documents with refine approach\n",
    "    for i, doc in enumerate(docs[1:], start=2):\n",
    "        print(f\"Refining with document {i}/{len(docs)}...\")\n",
    "        \n",
    "        # Refine prompt\n",
    "        refine_prompt = \"\"\"Your job is to refine an existing summary.\n",
    "        We have an existing summary: {existing_summary}\n",
    "        \n",
    "        We have a new document to add information from: {text}\n",
    "        \n",
    "        Please update the summary to incorporate new information from the document.\n",
    "        If the document doesn't contain relevant information, return the existing summary.\n",
    "        \n",
    "        REFINED SUMMARY:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Apply resource optimization between requests\n",
    "            time.sleep(10.0)  # Base delay between requests\n",
    "            \n",
    "            # Update the summary\n",
    "            current_summary = llm(refine_prompt.format(\n",
    "                existing_summary=current_summary,\n",
    "                text=doc.page_content\n",
    "            ))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Successfully refined with document {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during refinement with document {i}: {e}\")\n",
    "            # Apply exponential backoff\n",
    "            backoff = min(10.0 * (2 ** (i % 5)) + (random.random() * 2), 30)\n",
    "            print(f\"Backing off for {backoff:.2f} seconds...\")\n",
    "            time.sleep(backoff)\n",
    "            \n",
    "            # Try one more time\n",
    "            try:\n",
    "                current_summary = llm(refine_prompt.format(\n",
    "                    existing_summary=current_summary,\n",
    "                    text=doc.page_content\n",
    "                ))\n",
    "            except Exception as retry_error:\n",
    "                print(f\"Retry failed for document {i}: {retry_error}\")\n",
    "                # Continue with current summary rather than failing completely\n",
    "    \n",
    "    return {\"output_text\": current_summary}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d2892-54d2-43a1-a69c-58a45432cfe1",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The manual implementation gives you more control over:\n",
    "\n",
    "- The exact prompts used for summarization\n",
    "- Error handling and recovery\n",
    "- Resource optimization between API calls\n",
    "- Graceful degradation when errors occur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c914f0-cbc3-48b4-a9ee-f14d02ce921e",
   "metadata": {},
   "source": [
    "## Task 2b.6: Main Execution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e4d11-a876-4bd9-9d91-404017452916",
   "metadata": {},
   "source": [
    "Now we'll create a main function that orchestrates the entire document summarization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819efba-6a3f-4aaa-8668-13b45993af10",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The main function (`summarize_document`) allows you to choose which implementation to use based on the `chain_type` parameter, making it easy to compare results and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f3d0718-2adf-4bf9-bf18-acf4644115d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:17.231233Z",
     "iopub.status.busy": "2025-09-28T02:52:17.230926Z",
     "iopub.status.idle": "2025-09-28T02:52:17.236869Z",
     "shell.execute_reply": "2025-09-28T02:52:17.235934Z",
     "shell.execute_reply.started": "2025-09-28T02:52:17.231209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def summarize_document(file_path, chunk_size=4000, chain_type=\"refine\"):\n",
    "    \"\"\"Main function to summarize a document.\"\"\"\n",
    "    \n",
    "    print(f\"Starting document summarization process for: {file_path}\")\n",
    "    \n",
    "    # Load the document\n",
    "    document_text = load_document(file_path)\n",
    "    if not document_text:\n",
    "        return \"Failed to load document.\"\n",
    "    \n",
    "    print(f\"Document loaded successfully. Length: {len(document_text)} characters\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    docs = chunk_document(document_text, chunk_size=chunk_size, chunk_overlap=200)\n",
    "    \n",
    "    # If document is very large, provide a warning\n",
    "    if len(docs) > 15:\n",
    "        print(f\"Warning: Document is large ({len(docs)} chunks). Processing may take some time.\")\n",
    "        \n",
    "        # For very large documents, consider using a subset for testing\n",
    "        if len(docs) > 30:\n",
    "            print(\"Document is extremely large. Consider using a smaller chunk_size or processing a subset.\")\n",
    "            # Optional: process only a subset for testing\n",
    "            # docs = docs[:15]\n",
    "    \n",
    "    # Process the documents\n",
    "    print(f\"Processing document using '{chain_type}' chain type...\")\n",
    "    \n",
    "    # Use the appropriate processing method based on chain type\n",
    "    if chain_type == \"refine\":\n",
    "        # Use our manual implementation for better control over resource optimization\n",
    "        result = manual_refine_with_optimization(docs, resource_optimized_llm)\n",
    "    else:\n",
    "        # Use standard LangChain implementation for other chain types\n",
    "        result = process_documents_with_pacing(docs, chain_type=chain_type)\n",
    "    \n",
    "    # Return the result\n",
    "    if result and \"output_text\" in result:\n",
    "        print(\"\\nSummarization completed successfully!\")\n",
    "        return result[\"output_text\"]\n",
    "    else:\n",
    "        print(\"\\nSummarization failed or returned no result.\")\n",
    "        return \"Summarization process did not produce a valid result.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfca15-b18e-4aba-8a09-2500fc941f8e",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** Depending on your number of documents, Bedrock request rate quota, and configured retry settings - the summarization process may take some time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a31741-103d-4524-b3a8-3c68da2baa07",
   "metadata": {},
   "source": [
    "## Task 2b.7: Run the Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0de4dd-9430-46fe-ae9f-8af121f41b51",
   "metadata": {},
   "source": [
    "Let's run the summarization on the shareholder letter. By default, the summarize_document() function uses the refine chain. To enable map_reduce: \n",
    "\n",
    "- Comment out the following line: `summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")`\n",
    "- Uncomment the following line: `# summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42a88c-44af-4b35-8eed-d8ecd3ea38df",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** Don't worry if you notice error messages during execution. Your code includes robust error handling that will automatically retry failed requests with exponential backoff. This is normal behavior when working with service quotas and demonstrates how production-ready applications should handle API limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d2ffd26-bfbd-47d1-b6d7-76daabc8e952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T02:52:18.659490Z",
     "iopub.status.busy": "2025-09-28T02:52:18.659217Z",
     "iopub.status.idle": "2025-09-28T02:56:13.629732Z",
     "shell.execute_reply": "2025-09-28T02:56:13.628852Z",
     "shell.execute_reply.started": "2025-09-28T02:52:18.659469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document summarization process for: ../letters/2022-letter.txt\n",
      "Document loaded successfully. Length: 32438 characters\n",
      "Document split into 10 chunks\n",
      "Processing document using 'refine' chain type...\n",
      "Processing initial document (1/10)...\n",
      "Making API call (attempt 1/10)...\n",
      "Initial summary created successfully.\n",
      "Refining with document 2/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 2\n",
      "Refining with document 3/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 3\n",
      "Refining with document 4/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 4\n",
      "Refining with document 5/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 5\n",
      "Refining with document 6/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 6\n",
      "Refining with document 7/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 7\n",
      "Refining with document 8/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 8\n",
      "Refining with document 9/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 9\n",
      "Refining with document 10/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 10\n",
      "\n",
      "Summarization completed successfully!\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "\n",
      "**Refined Summary:**\n",
      "\n",
      "In his second annual shareholder letter, Amazon CEO reflects on the company's resilience and growth despite challenging macroeconomic conditions in 2022. Despite significant operating challenges, Amazon managed to grow demand, innovate across its largest businesses, and adjust investment strategies while maintaining long-term commitments. The company has evolved from a small online bookstore to a global retail giant with diverse product offerings, successfully venturing into cloud computing with AWS, digital reading with Kindle, and smart home technology with Alexa.\n",
      "\n",
      "Amazon has historically navigated economic downturns by balancing cost-cutting measures with strategic investments in future growth areas. For instance, Amazon's investment in AWS has grown into an $85 billion annual revenue business with strong profitability, even during the 2008-2009 recession.\n",
      "\n",
      "More recently, Amazon has taken a rigorous and adaptive approach to managing its business. By evaluating each initiative's long-term potential, the company has made tough decisions, such as shutting down certain physical store concepts, closing initiatives like Amazon Fabric and Amazon Care, and discontinuing some devices that did not show a path to meaningful returns. These strategic adjustments also led to the difficult decision to eliminate 27,000 corporate roles and implement other cost-saving measures.\n",
      "\n",
      "The CEO remains optimistic about Amazon's future prospects, emphasizing the importance of embracing change and the company's ability to respond proactively to new challenges. The company has also re-evaluated its remote work policies, deciding to bring corporate employees back to the office at least three days a week starting in May. The CEO believes that in-person collaboration is crucial for innovation and maintaining the company's unique culture.\n",
      "\n",
      "Amazon has tackled the rising cost to serve in its Stores fulfillment network by re-architecting its inventory placement strategy, moving from a national to a regionalized network model. This change has resulted in shorter travel distances, lower costs, less environmental impact, and faster delivery times, with Amazon on track to achieve its fastest Prime delivery speeds ever in 2023.\n",
      "\n",
      "AWS, with an $85 billion annualized revenue run rate, continues to innovate rapidly, launching over 3,300 new features and services in 2022. AWS is prioritizing customer relationships and long-term value, helping customers optimize their AWS spend to better weather the uncertain economy. This customer-focused approach has been appreciated by many AWS customers, who are not just cost-cutting but cost-optimizing to apply their resources to emerging and inventive new customer experiences.\n",
      "\n",
      "Amazon’s Advertising business is also uniquely effective for brands, contributing to its rapid growth. Amazon's sponsored products and brands offerings are tailored to customer search behaviors, leading to more relevant and effective advertising. This has resulted in a 25% YoY growth overall for 2022 on a $31B revenue base.\n",
      "\n",
      "Amazon strives to be the best place for advertisers to build their brands. The company has made large investments in machine learning to hone its advertising selection algorithms and has built comprehensive, flexible, and durable planning and measurement solutions, giving marketers greater insight into advertising effectiveness. An example is Amazon Marketing Cloud (“AMC”), a secure digital environment in which advertisers can run custom audience and campaign analytics across a range of first and third-party inputs, in a privacy-safe manner, to generate advertising and business insights.\n",
      "\n",
      "When evaluating new investment opportunities, Amazon asks itself a few questions: If we were successful, could it be big and have a reasonable return on invested capital? Is the opportunity being well-served today? Do we have a differentiated approach? And, do we have competence in that area? And if not, can we acquire it quickly? If we like the answers to those questions, then we’ll invest. This process has led to some expansions that seem straightforward, and others that some folks might not have initially guessed.\n",
      "\n",
      "Amazon has expanded from selling just books to adding categories like Music, Video, Electronics, and Toys. The company has also invested in new international geographies, including India, Brazil, Mexico, Australia, various European countries, the Middle East, and parts of Africa. This international expansion drove $118 billion in revenue in 2022.\n",
      "\n",
      "Beyond geographic expansion, Amazon has been working on expanding its customer offerings across large, unique product retail market segments. Grocery is a significant market segment in the US, with Amazon offering over three million items compared to a typical supermarket’s 30,000 for the same categories. Amazon Business drives roughly $35 billion in annualized gross sales and has over six million active customers, including 96 of the global Fortune 100 companies.\n",
      "\n",
      "Amazon is also expanding its services to include healthcare, with Amazon Pharmacy offering transparent pricing, easy refills, and savings for Prime members. The launch of RxPass, which for a $5 per month flat fee, enables Prime members to get as many of the eligible prescription medications as they need for dozens of common conditions, aims to provide a better alternative to the broader healthcare experience. Amazon's acquisition of One Medical in July 2022 furthers this goal by offering a patient-focused experience with a digital app, easy access to medical practitioners, and relationships with specialty physicians and local hospital systems.\n",
      "\n",
      "Additionally, Amazon has introduced Buy with Prime to help third-party brands and sellers drive conversion from views to purchases. This initiative has increased shopper conversion on third-party shopping sites by 25% on average.\n",
      "\n",
      "Kuiper is another example of Amazon innovating for customers over the long term in an area where there’s high customer need. Amazon’s vision for Kuiper is to create a low-Earth orbit satellite system to deliver high-quality broadband internet service to places around the world that don’t currently have it. Kuiper aims to provide affordable and accessible internet to hundreds of millions of households and businesses. The system will use low-cost antennas that will lower the barriers to access, with residential versions expected to cost less than $400 each. Kuiper’s terminals are compact, delivering speeds up to 400 megabits per second, and are powered by Amazon-designed baseband chips. Amazon plans to launch two prototype satellites to test the network this year and begin beta operations with commercial customers in 2024. The customer reaction to Kuiper has been very positive, indicating a potentially large opportunity for Amazon.\n",
      "\n",
      "One final investment area that is core to setting Amazon up to invent in every area of its business for many decades to come, and where Amazon is investing heavily, is Large Language Models (LLMs) and Generative AI. Machine learning has been a technology with high promise for several decades, but it’s only been the last five to ten years that it’s started to be used more pervasively by companies. This shift was driven by several factors, including access to higher volumes of compute capacity at lower prices than was ever available. Amazon has been using machine learning extensively for 25 years, employing it in everything from personalized ecommerce recommendations, to fulfillment center pick paths, to drones for Prime Air, to Alexa, to the many machine learning services AWS offers. More recently, a newer form of machine learning, called Generative AI, has burst onto the scene and promises to significantly accelerate machine learning adoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billions of parameters, and growing), across expansive datasets, and has radically general and broad recall and learning capabilities. Amazon has been working on its own LLMs for a while now, believes it will transform and improve virtually every customer experience, and will continue to invest substantially in these models across all of its consumer, seller, brand, and creator experiences. Additionally, as it has done for years in AWS, Amazon is democratizing this technology so companies of all sizes can leverage Generative AI. AWS is offering the most price-performant machine learning chips in Trainium and Inferentia so small and large companies can afford to train and run their LLMs in production. Amazon enables companies to choose from various LLMs and build applications with all of the AWS security, privacy, and other features that customers are accustomed to using. And, it is delivering applications like AWS’s CodeWhisperer, which revolutionizes developer productivity by generating code suggestions in real time.\n",
      "\n",
      "Amazon remains committed to expanding internationally, pursuing large retail market segments, and leveraging its unique assets to help merchants sell more effectively on their own websites. The company is confident in its plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy operating margins.\n",
      "\n",
      "In closing, the CEO is optimistic that Amazon will emerge from this challenging macroeconomic time in a stronger position than when it entered it. There are several reasons for this optimism, and many of them have been mentioned above. However, two relatively simple statistics underline Amazon's immense future opportunity. While Amazon has a consumer business that’s $434 billion in 2022, the vast majority of total market share in global retail still resides in physical stores (roughly 80%). Similarly, for Global IT spending, Amazon has AWS revenue of $80 billion in 2022, with about 90% of Global IT spending still on-premises and yet to migrate to the cloud. As these equations steadily flip—as Amazon is already seeing happen—the company believes its leading customer experiences, relentless invention, customer focus, and hard work will result in significant growth in the coming years. And, of course, this doesn’t include the other businesses and experiences Amazon is pursuing, all of which are still in their early days.\n",
      "\n",
      "The CEO strongly believes that Amazon's best days are in front of it, and looks forward to working with his teammates to make it so.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your document\n",
    "    document_path = \"../letters/2022-letter.txt\"\n",
    "    \n",
    "    # Summarize with different options\n",
    "    # Option 1: Standard refine chain (sequential processing, good for resource optimization)\n",
    "    summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")\n",
    "    \n",
    "    # Option 2: For comparison, you could try map_reduce (but be careful with service quotas)\n",
    "    # summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")\n",
    "    \n",
    "    # Print the final summary\n",
    "    print(\"\\n=== FINAL SUMMARY ===\\n\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21988a-db39-4511-a426-b21feb75d2f2",
   "metadata": {},
   "source": [
    "You have now experimented with using prompt chunking and chaining with the LangChain framework to summarize large documents while mitigating issues arising from long input text.\n",
    "\n",
    "## Understanding the Key Components\n",
    "\n",
    "Let's review the key components of our solution:\n",
    "\n",
    "1. **Resource Optimization**: The `ResourceOptimizedLLM` wrapper manages API calls to stay within Bedrock service quotas by:\n",
    "   - Adding pauses between requests (controlled by `initial_pause`)\n",
    "   - Implementing exponential backoff with jitter when throttling occurs\n",
    "   - Providing comprehensive error handling and recovery\n",
    "\n",
    "2. **Document Chunking**: The `chunk_document` function splits large documents into manageable pieces:\n",
    "   - `chunk_size` controls the maximum size of each chunk (4000 characters)\n",
    "   - `chunk_overlap` ensures context continuity between chunks (200 characters)\n",
    "   - Natural text separators (`\\n\\n`, `\\n`, `.`, etc.) are used to avoid breaking mid-paragraph\n",
    "\n",
    "3. **Summarization Approaches**:\n",
    "   - **Refine Chain**: Processes chunks sequentially, refining the summary with each new chunk\n",
    "   - **Map-Reduce**: Summarizes each chunk independently, then combines and summarizes those summaries\n",
    "\n",
    "4. **Error Handling**: Comprehensive error handling ensures the process can recover from:\n",
    "   - Service throttling and capacity limits\n",
    "   - Access permission issues\n",
    "   - Other API errors\n",
    "\n",
    "## Try it yourself\n",
    "\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Experiment with different chunk sizes to find the optimal balance between context preservation and processing efficiency.\n",
    "- Try different summarization chain types (`refine` vs `map_reduce`) and compare the results.\n",
    "- Adjust the resource optimization parameters based on your Bedrock quota limits.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "This approach can be applied to summarize various types of long-form content:\n",
    "- Customer service call transcripts\n",
    "- Meeting transcripts and notes\n",
    "- Research papers and technical documents\n",
    "- Legal documents and contracts\n",
    "- Books, articles, and blog posts\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "When implementing this solution in production:\n",
    "\n",
    "1. **Monitor API usage**: Keep track of your API calls to stay within quota limits\n",
    "2. **Optimize chunk size**: Balance between context preservation and processing efficiency\n",
    "3. **Implement proper error handling**: Ensure your application can gracefully handle API errors\n",
    "4. **Consider caching**: Cache results to avoid redundant API calls for frequently accessed documents\n",
    "5. **Test with various document types**: Different content may require different chunking strategies\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with **Task 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabb8b5-6f25-4a6a-a7d7-f4b8b86f9e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
